# LLM_bias_mitigation
Overview
This repository explores various techniques for mitigating biases in Large Language Models (LLMs) to ensure fair and ethical AI-generated responses. The project implements multiple debiasing strategies, including:

DPO (Direct Preference Optimization) for bias mitigation
Bias evaluation metrics to assess fairness in models
LoRA (Low-Rank Adaptation) for efficient fine-tuning
MEMIT (Mass-Editing Memory in Transformers) for targeted bias correction
REPE (Retrieval-Augmented Prompt Engineering) to refine model responses
Features
Comprehensive Bias Evaluation – Tools to measure biases before and after mitigation
DPO-Based Fine-Tuning – Preference-based training to improve fairness
LoRA Adaptation – Lightweight, efficient fine-tuning for targeted corrections
MEMIT Editing – Controlling memory to reduce biased outputs
REPE Strategies – Prompt engineering to counteract model biases
